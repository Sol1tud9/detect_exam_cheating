import streamlit as st
import streamlit.components.v1 as components
import cv2
import os
import pandas as pd
import numpy as np
from pathlib import Path
import tempfile
from ultralytics import YOLO
import torch
from datetime import datetime
import time
import glob
import shutil
from collections import defaultdict, deque
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
from streamlit_player import st_player
import copy


//—Å—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
class PhoneDetectorResNet50(nn.Module):
    def __init__(self, pretrained=True):
        super(PhoneDetectorResNet50, self).__init__()
        self.resnet = models.resnet50(pretrained=pretrained)
        self.features = nn.Sequential(*list(self.resnet.children())[:-1])
        self.phone_classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(), 
            nn.Dropout(0.5), 
            nn.Linear(2048, 512), 
            nn.ReLU(), 
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 1), 
            nn.Sigmoid() 
        )
    
    def forward(self, x):
        features = self.features(x)
        phone_score = self.phone_classifier(features)
        return phone_score

class ObjectTracker:
    def __init__(self, max_disappeared=30, max_distance=150):
        self.next_id = 0
        self.objects = {}
        self.disappeared = {}
        self.max_disappeared = max_disappeared
        self.max_distance = max_distance
        
    def register(self, bbox, centroid, class_name, confidence):
        self.objects[self.next_id] = {
            'bbox': bbox,
            'centroid': centroid,
            'class': class_name,
            'confidence': confidence,
            'history': deque(maxlen=20),
            'first_seen': datetime.now(),
            'color': self._generate_color()
        }
        self.objects[self.next_id]['history'].append(centroid)
        self.disappeared[self.next_id] = 0
        self.next_id += 1
        
    def _generate_color(self):
        np.random.seed(self.next_id)
        return tuple(map(int, np.random.randint(0, 255, 3)))
        
    def deregister(self, object_id):
        if object_id in self.objects:
            del self.objects[object_id]
        if object_id in self.disappeared:
            del self.disappeared[object_id]
        
    def update(self, detections):
        if len(detections) == 0:
            for object_id in list(self.disappeared.keys()):
                self.disappeared[object_id] += 1
                if self.disappeared[object_id] > self.max_disappeared:
                    self.deregister(object_id)
            return self.objects
        
        if len(self.objects) == 0:
            for detection in detections:
                bbox, centroid, class_name, confidence = detection
                self.register(bbox, centroid, class_name, confidence)
        else:
            object_centroids = [obj['centroid'] for obj in self.objects.values()]
            object_ids = list(self.objects.keys())
            
            detection_centroids = [det[1] for det in detections]

            if len(object_centroids) > 0 and len(detection_centroids) > 0:
                D = np.linalg.norm(np.array(object_centroids)[:, np.newaxis] - 
                                 np.array(detection_centroids), axis=2)

                rows = D.min(axis=1).argsort()
                cols = D.argmin(axis=1)[rows]
                
                used_row_indices = set()
                used_col_indices = set()
                
                for (row, col) in zip(rows, cols):
                    if row in used_row_indices or col in used_col_indices:
                        continue
                        
                    if D[row, col] > self.max_distance:
                        continue
                        
                    object_id = object_ids[row]
                    bbox, centroid, class_name, confidence = detections[col]
                    
                    self.objects[object_id]['bbox'] = bbox
                    self.objects[object_id]['centroid'] = centroid
                    self.objects[object_id]['class'] = class_name
                    self.objects[object_id]['confidence'] = confidence
                    self.objects[object_id]['history'].append(centroid)
                    self.disappeared[object_id] = 0
                    
                    used_row_indices.add(row)
                    used_col_indices.add(col)
                
                unused_rows = set(range(0, D.shape[0])).difference(used_row_indices)
                unused_cols = set(range(0, D.shape[1])).difference(used_col_indices)
                
                if D.shape[0] >= D.shape[1]:
                    for row in unused_rows:
                        object_id = object_ids[row]
                        self.disappeared[object_id] += 1
                        
                        if self.disappeared[object_id] > self.max_disappeared:
                            self.deregister(object_id)
                else:
                    for col in unused_cols:
                        bbox, centroid, class_name, confidence = detections[col]
                        self.register(bbox, centroid, class_name, confidence)
        
        return self.objects

class RealTimeVideoObjectDetector:
    def __init__(self, person_classifier_model_path, yolo_phone_model_path=None, yolo_model='yolov8n.pt',
                 yolo_conf_detection=0.3, person_classification_conf=0.5, phone_detection_conf=0.5):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        self.yolo_model = YOLO(yolo_model)
        self.yolo_model.to(self.device)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ YOLO –º–æ–¥–µ–ª—å –¥–ª—è –ª—é–¥–µ–π: {yolo_model}")

        self.phone_yolo_model = None
        if yolo_phone_model_path and os.path.exists(yolo_phone_model_path):
            self.phone_yolo_model = YOLO(yolo_phone_model_path)
            self.phone_yolo_model.to(self.device)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–∞—Å—Ç–æ–º–Ω–∞—è YOLO –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {yolo_phone_model_path}")

        self.person_classifier_model = None
        if person_classifier_model_path and os.path.exists(person_classifier_model_path):
            self.person_classifier_model = YOLO(person_classifier_model_path)
            self.person_classifier_model.to(self.device)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ YOLO –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª—é–¥–µ–π: {person_classifier_model_path}")

        self.yolo_conf_detection = yolo_conf_detection
        self.person_classification_conf = person_classification_conf
        self.phone_detection_conf = phone_detection_conf
        print(f"–ü–æ—Ä–æ–≥–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: YOLO –¥–µ—Ç–µ–∫—Ü–∏—è={self.yolo_conf_detection}, –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ–ª–æ–≤–µ–∫–∞={self.person_classification_conf}, –î–µ—Ç–µ–∫—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–∞={self.phone_detection_conf}")
        
        self.tracker = ObjectTracker(max_disappeared=30, max_distance=150)
        
        self.colors = {
            'student': (0, 255, 0),
            'teacher': (255, 0, 0),
            'phone': (0, 0, 255),
            'person': (255, 255, 0)
        }
        
        self.detection_stats = defaultdict(int)
        self.frame_count = 0
        self.fps_counter = deque(maxlen=30)
        
    def detect_objects_in_frame(self, frame):
        detections = []
        
        person_results = self.yolo_model(frame, conf=self.yolo_conf_detection, classes=[0], verbose=False)

        for result in person_results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    confidence = box.conf[0].cpu().numpy()
                    
                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                    person_roi = frame[y1:y2, x1:x2]
                    
                    if person_roi.size == 0:
                        continue
                    
                    final_class = 'person'
                    final_confidence = confidence
                    
                    if self.person_classifier_model is not None:
                        _class_confidence = None
                        _predicted_index = None

                        try:
                            cls_results = self.person_classifier_model(person_roi, verbose=False)

                            if cls_results and len(cls_results) > 0 and cls_results[0] is not None and hasattr(cls_results[0], 'probs'):
                                probs = cls_results[0].probs

                                if probs is not None:
                                    if isinstance(probs, torch.Tensor):
                                        _class_confidence, _predicted_index = probs.max(0)
                                        _class_confidence = _class_confidence.item()
                                        _predicted_index = _predicted_index.item()
                                    elif hasattr(probs, 'top1conf') and hasattr(probs, 'top1'): # Assume Probs object if not Tensor
                                        _class_confidence = probs.top1conf
                                        _predicted_index = probs.top1
                                        if isinstance(_class_confidence, torch.Tensor):
                                            _class_confidence = _class_confidence.item()
                                    else:
                                        print(f"WARNING: Unexpected type or malformed probs object ({type(probs)}). Skipping detailed classification.")
                                else:
                                    print("WARNING: cls_results[0].probs returned None. Skipping detailed classification.")
                            else:
                                print("WARNING: No valid classification result object (cls_results is empty/None or missing .probs). Skipping detailed classification.")
                        except Exception as e:
                            print(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞: {e}")

                        if _class_confidence is not None and _class_confidence > self.person_classification_conf:
                            final_class = self.person_classifier_model.names[_predicted_index]
                            final_confidence = _class_confidence
                    
                    centroid = (int((x1 + x2) / 2), int((y1 + y2) / 2))
                    bbox = (x1, y1, x2, y2)
                    detections.append((bbox, centroid, final_class, final_confidence))
        
        if self.phone_yolo_model is not None:
            phone_results = self.phone_yolo_model(frame, conf=self.phone_detection_conf, verbose=False)
            for result in phone_results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        confidence = box.conf[0].cpu().numpy()
                        
                        centroid = (int((x1 + x2) / 2), int((y1 + y2) / 2))
                        bbox = (int(x1), int(y1), int(x2), int(y2))
                        detections.append((bbox, centroid, 'phone', confidence))

        return detections
    
    def draw_tracking_info(self, frame, objects):
        for object_id, obj_info in objects.items():
            bbox = obj_info['bbox']
            centroid = obj_info['centroid']
            class_name = obj_info['class']
            confidence = obj_info['confidence']
            
            color = self.colors.get(class_name, obj_info.get('color', (255, 255, 255)))
            
            x1, y1, x2, y2 = bbox
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            
            cv2.circle(frame, centroid, 5, color, -1)
            
            text = f"ID:{object_id} {class_name} ({confidence:.2f})"
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            
            cv2.rectangle(frame, (x1, y1 - text_size[1] - 10), 
                         (x1 + text_size[0], y1), color, -1)
            cv2.putText(frame, text, (x1, y1 - 5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            if len(obj_info['history']) > 1:
                points = list(obj_info['history'])
                for i in range(1, len(points)):
                    cv2.line(frame, points[i-1], points[i], color, 2)
        
        return frame


st.set_page_config(
    page_title="Exam Cheating",
    page_icon="üé•",
    layout="wide"
)

CONFIG = {
    "STEP": 15,      
    "CONF": 0.3,
    "MAX_CROPS_PER_ID": 120  
}

def convert_df_to_csv(df):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç DataFrame –≤ CSV-–±–∞–π—Ç—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ Streamlit."""
    return df.to_csv(index=False).encode('utf-8')

def convert_df_to_html_table(df):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç DataFrame –≤ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é HTML-—Ç–∞–±–ª–∏—Ü—É –¥–ª—è Streamlit."""
    # –î–æ–±–∞–≤–ª—è–µ–º CSS-—Å—Ç–∏–ª–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞
    html_table = """
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        tr:hover {
            background-color: #f1f1f1;
        }
    </style>
    """
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –æ—Ç—á–µ—Ç–∞
    report_header = "<h2>–û—Ç—á–µ—Ç –æ –Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ</h2>"
    report_summary = f"<p>–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π: <strong>{len(df)}</strong></p>"
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –≤ HTML-—Ç–∞–±–ª–∏—Ü—É
    table_string = df.to_html(index=False)
    
    return (html_table + report_header + report_summary + table_string).encode('utf-8')

def build_dataset(crops_dir, labels_file, dataset_dir, target_size=(150, 150)):
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª—é–¥–µ–π (—Å—Ç—É–¥–µ–Ω—Ç/–ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å) –ø–æ –∫—Ä–æ–ø–∞–º –∏ –º–µ—Ç–∫–∞–º.
    –ö–æ–ø–∏—Ä—É–µ—Ç –∏ —Ä–µ—Å–∞–π–∑–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ train/val –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    """
    for split in ['train', 'val']:
        for label in ['student', 'teacher']:
            Path(dataset_dir, split, label).mkdir(parents=True, exist_ok=True)
    
    df = pd.read_csv(labels_file)
    label_map = dict(zip(df.tid.astype(str), df.label))
    
    for img_path in glob.glob(os.path.join(crops_dir, "*.jpg")):
        tid = os.path.basename(img_path).split("_")[0]
        label = label_map.get(tid)
        if label:
            split = 'val' if np.random.random() < 0.15 else 'train'
            dst_path = os.path.join(dataset_dir, split, label, os.path.basename(img_path))
            shutil.copy(img_path, dst_path)

            img = Image.open(img_path)
            img = img.resize(target_size)
            img.save(dst_path)


def initialize_detector(person_classifier_model_path=None, yolo_phone_model_path=None, 
                        yolo_conf_det=0.3, person_class_conf=0.5, phone_det_conf=0.5): 
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ–±—ä–µ–∫—Ç–æ–≤ (–ª—é–¥–∏/—Ç–µ–ª–µ—Ñ–æ–Ω—ã) —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –ø–æ—Ä–æ–≥–∞–º–∏.
    –ö—ç—à–∏—Ä—É–µ—Ç –≤ —Å–µ—Å—Å–∏–∏ Streamlit –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤.
    """
    if 'detector' not in st.session_state or \
        st.session_state.get('person_model_path_loaded') != person_classifier_model_path or \
        st.session_state.get('phone_model_path_loaded') != yolo_phone_model_path or \
        st.session_state.get('yolo_conf_det_loaded') != yolo_conf_det or \
        st.session_state.get('person_class_conf_loaded') != person_class_conf or \
        st.session_state.get('phone_det_conf_loaded') != phone_det_conf:
        
        with st.spinner('–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞...'):
            try:
                detector = RealTimeVideoObjectDetector(
                    person_classifier_model_path=person_classifier_model_path,
                    yolo_phone_model_path=yolo_phone_model_path,  
                    yolo_model='yolov8n.pt',
                    yolo_conf_detection=yolo_conf_det,           
                    person_classification_conf=person_class_conf, 
                    phone_detection_conf=phone_det_conf          
                )
                st.session_state.detector = detector
                st.session_state.person_model_path_loaded = person_classifier_model_path
                st.session_state.phone_model_path_loaded = yolo_phone_model_path
                st.session_state.yolo_conf_det_loaded = yolo_conf_det
                st.session_state.person_class_conf_loaded = person_class_conf
                st.session_state.phone_det_conf_loaded = phone_det_conf

                if person_classifier_model_path:
                    st.success("‚úÖ –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª—é–¥–µ–π (YOLO) –∑–∞–≥—Ä—É–∂–µ–Ω–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∞")
                else:
                    st.info("‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª—é–¥–µ–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ YOLO –¥–ª—è –ª—é–¥–µ–π")
                
                if yolo_phone_model_path:
                    st.success("‚úÖ –ú–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ (YOLO) –∑–∞–≥—Ä—É–∂–µ–Ω–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∞")
                else:
                    st.info("‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –¥–µ—Ç–µ–∫—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å—Å—è –Ω–µ –±—É–¥–µ—Ç.")
                st.success(f"‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.")

            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞: {str(e)}")
                st.session_state.detector = None 
                return None
    elif st.session_state.detector is None: 
        st.error("‚ùå –î–µ—Ç–µ–∫—Ç–æ—Ä –Ω–µ –±—ã–ª —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Ä–∞–Ω–µ–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –º–æ–¥–µ–ª–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
        return None
        
    return st.session_state.detector

def bbox_iou(boxA, boxB):
    """–í—ã—á–∏—Å–ª—è–µ—Ç IoU (Intersection over Union) –º–µ–∂–¥—É –¥–≤—É–º—è bbox-–∞–º–∏."""
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])
    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)
    return iou

def extract_people_from_video(video_path, output_dir, tid_prefix, 
                              step=15, yolo_conf=0.3, max_crops=120):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ª—é–¥–µ–π –∏–∑ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é YOLO, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—Ä–æ–ø—ã –∏ –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
    step ‚Äî —à–∞–≥ –ø–æ –∫–∞–¥—Ä–∞–º, max_crops ‚Äî –º–∞–∫—Å–∏–º—É–º –∫—Ä–æ–ø–æ–≤ –Ω–∞ –æ–¥–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞.
    """
    Path(output_dir).mkdir(exist_ok=True, parents=True)
    model = YOLO('yolov8n.pt')
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    progress_bar = st.progress(0)
    status_text = st.empty()
    id2cnt = {}
    frame_i = 0
    meta = []
    
    processed_frames_count = 0 
    
    while True:
        ok, frame = cap.read()
        if not ok:
            break
        
        current_frame_for_progress = frame_i + 1 
        
        if frame_i % step: 
            frame_i += 1
            if frame_i % (step * 5) == 0 or current_frame_for_progress >= total_frames :
                 progress_bar.progress(current_frame_for_progress / total_frames)
            continue
        
        processed_frames_count +=1
        status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ {current_frame_for_progress}/{total_frames} (–∏–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ {processed_frames_count})")
        res = model.track(frame, conf=yolo_conf, classes=[0], persist=True, verbose=False)[0] 
        
        if res.boxes.id is not None:
            for box, tid in zip(res.boxes.xyxy.cpu().numpy(), res.boxes.id.cpu().numpy()):
                tid = int(tid)
                unique_tid = f"{tid_prefix}_{tid}"
                id2cnt.setdefault(unique_tid, 0)
                if id2cnt[unique_tid] >= max_crops: 
                    continue
                x1, y1, x2, y2 = map(int, box)
                crop = frame[y1:y2, x1:x2]
                if crop.size == 0:
                    continue
                fname = f"{unique_tid}_{frame_i}.jpg"
                cv2.imwrite(os.path.join(output_dir, fname), crop)
                meta.append({
                    'tid': unique_tid,
                    'frame': frame_i,
                    'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,
                    'img': fname
                })
                id2cnt[unique_tid] += 1
        frame_i += 1
        progress_bar.progress(current_frame_for_progress / total_frames)
    
    cap.release()
    status_text.text(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –≤–∏–¥–µ–æ ({os.path.basename(video_path)}) –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    return id2cnt, meta


def label_people(crops_dir, meta_csv='crops_meta.csv', labels_file='labels.csv'):
    """
    –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit –¥–ª—è —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –ª—é–¥–µ–π –∫–∞–∫ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π –∏–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ labels.csv.
    """
    st.subheader("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π")
    if not os.path.exists(meta_csv):
        st.warning("–ù–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏!")
        return None
    meta = pd.read_csv(meta_csv)
    all_ids = sorted(meta['tid'].unique())
    if not all_ids:
        st.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏")
        return None
    cols_per_row = 4
    selected_teachers = set()
    for i in range(0, len(all_ids), cols_per_row):
        cols = st.columns(cols_per_row)
        for j, col in enumerate(cols):
            if i + j < len(all_ids):
                tid = all_ids[i + j]
                first_img_row = meta[meta['tid']==tid]['img'].values
                if len(first_img_row) > 0:
                    img_path = os.path.join(crops_dir, first_img_row[0])
                    if os.path.exists(img_path):
                        img = Image.open(img_path)
                        img = img.resize((180, 180))
                        col.image(img, caption=f"ID: {tid}")
                        if col.checkbox("–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å", key=f"teacher_{tid}"):
                            selected_teachers.add(tid)
                    else:
                        col.warning(f"Img not found: {img_path}")
                else:
                    col.info(f"No images for ID {tid} in meta.")


    if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ç–∫—É"):
        meta['label'] = meta['tid'].apply(lambda tid: 'teacher' if tid in selected_teachers else 'student')
        meta.to_csv(labels_file, index=False)

        
        st.success("–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
        st.session_state.labeling_complete = True
        return meta
    return None


def load_bbox_labels(labels_file='labels.csv'):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç bbox-–º–µ—Ç–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞ labels.csv, –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ –≤–∏–¥–µ–æ –∏ –∫–∞–¥—Ä—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –º–µ—Ç–æ–∫ –ø–æ –∫–∞–¥—Ä—É.
    """
    by_video_and_frame = defaultdict(lambda: defaultdict(list))
    if os.path.exists(labels_file):
        df = pd.read_csv(labels_file)
        required_cols = ['frame', 'tid', 'x1', 'y1', 'x2', 'y2', 'label']
        if not all(col in df.columns for col in required_cols):
            st.error(f"–§–∞–π–ª –º–µ—Ç–æ–∫ {labels_file} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {required_cols}")
            return by_video_and_frame

        for _, row in df.iterrows():
            tid = str(row['tid'])
            video_prefix = tid.split('_')[0]
            frame_num = int(row['frame'])
            by_video_and_frame[video_prefix][frame_num].append({
                'tid': row['tid'],
                'bbox': (row['x1'], row['y1'], row['x2'], row['y2']),
                'label': row['label']
            })
    return by_video_and_frame


def match_label(frame_id, bbox, by_video_and_frame, video_prefix, iou_thr=0.5):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à—É—é –º–µ—Ç–∫—É (label) –¥–ª—è bbox –ø–æ IoU —Å—Ä–µ–¥–∏ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∫–∞–¥—Ä–µ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç label –∏ tid, –µ—Å–ª–∏ IoU –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞.
    """
    candidates = by_video_and_frame.get(video_prefix, {}).get(frame_id, [])
    best_iou = 0
    best_label = None
    best_tid = None
    for c in candidates:
        iou = bbox_iou(bbox, c['bbox'])
        if iou > best_iou:
            best_iou = iou
            best_label = c['label']
            best_tid = c['tid']
    if best_iou > iou_thr:
        return best_label, best_tid
    return None, None


def process_video_frame(frame, detector, frame_idx=None, bbox_labels_by_video_and_frame=None, video_prefix=None, tid_manual_labels_map=None):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–¥—Ä –≤–∏–¥–µ–æ: –¥–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç—ã, —Ç—Ä–µ–∫–∞–µ—Ç –∏—Ö, —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π,
    —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞—Ä—É—à–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞–π–¥–µ–Ω —Ç–µ–ª–µ—Ñ–æ–Ω —É —á–µ–ª–æ–≤–µ–∫–∞).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞–¥—Ä —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏, —Ç—Ä–µ–∫–∏ –∏ –Ω–æ–≤—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è.
    """
    detections = detector.detect_objects_in_frame(frame)
    tracked_objects = detector.tracker.update(detections)

    new_violations_this_frame = []

    for obj_id, obj_info in tracked_objects.items():
        current_class_by_model = obj_info.get('class', 'unknown') 
        obj_bbox = obj_info['bbox']
        if current_class_by_model == 'phone':
            phone_confidence = obj_info.get('confidence', 0.0)
            violation_type = '–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –Ω–∞—Ä—É—à–µ–Ω–∏–µ'
            if phone_confidence >= 0.6 and phone_confidence < 0.9:
                violation_type = '–≤–æ–∑–º–æ–∂–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞'
            elif phone_confidence >= 0.9:
                violation_type = '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–∞'

            phone_details = {
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'frame': frame_idx,
                'type': violation_type,
                'phone_tracker_id': obj_id,
                'phone_bbox': obj_bbox,
                'phone_confidence': phone_confidence,
                'associated_person_tracker_id': None,
                'associated_person_class': None,
                'associated_person_bbox': None,
                'raw_model_class_for_phone_obj': current_class_by_model 
            }
            for p_obj_id, p_obj_info in tracked_objects.items():
                if p_obj_id == obj_id:
                    continue
                p_class = p_obj_info.get('class', 'unknown')
                if p_class in ['student', 'teacher', 'person']: 
                    iou_with_person = bbox_iou(obj_bbox, p_obj_info['bbox'])
                    phone_centroid_x = (obj_bbox[0] + obj_bbox[2]) / 2
                    phone_centroid_y = (obj_bbox[1] + obj_bbox[3]) / 2
                    person_x1, person_y1, person_x2, person_y2 = p_obj_info['bbox']
                    
                    is_inside = (person_x1 <= phone_centroid_x <= person_x2 and
                                 person_y1 <= phone_centroid_y <= person_y2)

                    if iou_with_person > 0.05 or is_inside: 
                        phone_details['associated_person_tracker_id'] = p_obj_id
                        phone_details['associated_person_class'] = p_class 
                        phone_details['associated_person_bbox'] = p_obj_info['bbox']
                        break 
            new_violations_this_frame.append(phone_details)

        if tid_manual_labels_map is not None:
            if obj_id in tid_manual_labels_map:
                manual_label = tid_manual_labels_map[obj_id]
                if manual_label in ['student', 'teacher'] and current_class_by_model in ['student', 'teacher', 'person']:
                    obj_info['class'] = manual_label
            else:
                if frame_idx is not None and bbox_labels_by_video_and_frame is not None and video_prefix is not None:
                    label_from_iou, original_tid_from_label_file = match_label(frame_idx, obj_bbox, bbox_labels_by_video_and_frame, video_prefix) 
                    
                    if label_from_iou and label_from_iou in ['student', 'teacher']:
                        if current_class_by_model in ['student', 'teacher', 'person']:
                            obj_info['class'] = label_from_iou

                            tid_manual_labels_map[obj_id] = label_from_iou 
                            
                            for viol in new_violations_this_frame:
                                if viol['associated_person_tracker_id'] == obj_id:
                                    viol['associated_person_class'] = label_from_iou


    frame_with_tracking = detector.draw_tracking_info(frame.copy(), tracked_objects)
    return frame_with_tracking, tracked_objects, new_violations_this_frame


def sync_yolo_slider():
    st.session_state.extraction_yolo_conf_input = st.session_state.extraction_yolo_conf_slider

def sync_yolo_input():
    st.session_state.extraction_yolo_conf_slider = st.session_state.extraction_yolo_conf_input

def sync_max_crops_slider():
    st.session_state.extraction_max_crops_input = st.session_state.extraction_max_crops_slider

def sync_max_crops_input():
    st.session_state.extraction_max_crops_slider = st.session_state.extraction_max_crops_input

def sync_slider():
    st.session_state.step_input = st.session_state.step_slider

def sync_input():
    st.session_state.step_slider = st.session_state.step_input

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Streamlit-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ, –º–æ–¥–µ–ª–µ–π, —Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –¥–µ—Ç–µ–∫—Ü–∏–∏.
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –≤–∫–ª–∞–¥–∫–∞–º–∏, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–∏–¥–µ–æ.
    """
    st.title("üé• –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–∞—Ä—É—à–µ–Ω–∏–π –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ")
    
    if 'extraction_step' not in st.session_state:
        st.session_state.extraction_step = CONFIG["STEP"]
    if 'extraction_yolo_conf' not in st.session_state:
        st.session_state.extraction_yolo_conf = CONFIG["CONF"]
    if 'extraction_max_crops' not in st.session_state:
        st.session_state.extraction_max_crops = CONFIG["MAX_CROPS_PER_ID"]
    
    if 'detection_yolo_conf' not in st.session_state:
        st.session_state.detection_yolo_conf = 0.3 
    if 'person_classification_conf' not in st.session_state:
        st.session_state.person_classification_conf = 0.5
    if 'phone_detection_conf' not in st.session_state:
        st.session_state.phone_detection_conf = 0.5

    if 'step_slider' not in st.session_state:
        st.session_state.step_slider = 15
    if 'step_input' not in st.session_state:
        st.session_state.step_input = 15
    if 'extraction_yolo_conf_slider' not in st.session_state:
        st.session_state.extraction_yolo_conf_slider = CONFIG["CONF"]
    if 'extraction_yolo_conf_input' not in st.session_state:
        st.session_state.extraction_yolo_conf_input = CONFIG["CONF"]
    if 'extraction_max_crops_slider' not in st.session_state:
        st.session_state.extraction_max_crops_slider = 120
    if 'extraction_max_crops_input' not in st.session_state:
        st.session_state.extraction_max_crops_input = 120

    tab1, tab2, tab3 = st.tabs(["üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ù–∞—Å—Ç—Ä–æ–π–∫–∏", "üè∑Ô∏è –†–∞–∑–º–µ—Ç–∫–∞", "üéØ –î–µ—Ç–µ–∫—Ü–∏—è"])
    
    with tab1:
        st.header("–ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ –∏ –º–æ–¥–µ–ª–µ–π")
        col1a, col2a = st.columns(2)
        with col1a:
            st.subheader("üìπ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ (–ø–µ—Ä–µ–¥–Ω–∏–π –ø–ª–∞–Ω –∞—É–¥–∏—Ç–æ—Ä–∏–∏)")
            max_video_size = 1024 * 1024 * 1024  # 1 –ì–ë
            video_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª (–¥–æ 1 –ì–ë)", type=['mp4', 'avi', 'mov', 'mkv'], key='front_uploader')
            if video_file:
                if video_file.size > max_video_size:
                    st.error("‚ùå –†–∞–∑–º–µ—Ä –≤–∏–¥–µ–æ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 1 –ì–ë. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.")
                elif not hasattr(st.session_state, 'video_path') or st.session_state.video_path is None or st.session_state.get('uploaded_video_name_front') != video_file.name:
                    tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
                    tfile.write(video_file.read())
                    st.session_state.video_path = tfile.name
                    st.session_state.uploaded_video_name_front = video_file.name
                    st.session_state.extraction_complete = False 
                    st.session_state.labeling_complete = False 
                    st.success(f"‚úÖ –í–∏–¥–µ–æ '{video_file.name}' –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                else:
                    st.info(f"–í–∏–¥–µ–æ '{st.session_state.uploaded_video_name_front}' —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")

            st.subheader("üìπ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ (–∑–∞–¥–Ω–∏–π –ø–ª–∞–Ω –∞—É–¥–∏—Ç–æ—Ä–∏–∏)")
            video_file_1 = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª (–¥–æ 1 –ì–ë)", type=['mp4', 'avi', 'mov', 'mkv'], key='back_uploader')
            if video_file_1:
                if video_file_1.size > max_video_size:
                    st.error("‚ùå –†–∞–∑–º–µ—Ä –≤–∏–¥–µ–æ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 1 –ì–ë. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.")
                elif not hasattr(st.session_state, 'video_path_1') or st.session_state.video_path_1 is None or st.session_state.get('uploaded_video_name_back') != video_file_1.name:
                    tfile_1 = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
                    tfile_1.write(video_file_1.read())
                    st.session_state.video_path_1 = tfile_1.name
                    st.session_state.uploaded_video_name_back = video_file_1.name
                    st.session_state.extraction_complete = False
                    st.session_state.labeling_complete = False
                    st.success(f"‚úÖ –í–∏–¥–µ–æ '{video_file_1.name}' –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                else:
                    st.info(f"–í–∏–¥–µ–æ '{st.session_state.uploaded_video_name_back}' —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
        
        with col2a:
            st.subheader("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")
            person_model_file = st.file_uploader("–ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å/–°—Ç—É–¥–µ–Ω—Ç, YOLO .pt)", type=['pt'])
            if person_model_file:
                if not hasattr(st.session_state, 'person_model_path') or st.session_state.person_model_path is None or st.session_state.get('uploaded_person_model_name') != person_model_file.name:
                    try:
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.pt') as tmp:
                            tmp.write(person_model_file.read())
                            st.session_state.person_model_path = tmp.name
                            st.session_state.uploaded_person_model_name = person_model_file.name
                        st.success(f"‚úÖ –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ '{person_model_file.name}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
                    except Exception as e:
                        st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {str(e)}")
                        st.session_state.person_model_path = None
                else:
                    st.info(f"–ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ '{st.session_state.uploaded_person_model_name}' —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
            
            phone_model_file = st.file_uploader("–ú–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ (YOLO .pt)", type=['pt'])
            if phone_model_file:
                if not hasattr(st.session_state, 'phone_model_path') or st.session_state.phone_model_path is None or st.session_state.get('uploaded_phone_model_name') != phone_model_file.name:
                    try:
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.pt') as tmp:
                            tmp.write(phone_model_file.read())
                            st.session_state.phone_model_path = tmp.name
                            st.session_state.uploaded_phone_model_name = phone_model_file.name
                        st.success(f"‚úÖ –ú–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ '{phone_model_file.name}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
                    except Exception as e:
                        st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {str(e)}")
                        st.session_state.phone_model_path = None
                else:
                    st.info(f"–ú–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ '{st.session_state.uploaded_phone_model_name}' —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

        video_paths_to_process = []
        if hasattr(st.session_state, 'video_path') and st.session_state.video_path:
            video_paths_to_process.append(st.session_state.video_path)
        if hasattr(st.session_state, 'video_path_1') and st.session_state.video_path_1:
            video_paths_to_process.append(st.session_state.video_path_1)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∏–¥–µ–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ
        if video_paths_to_process:
            st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –≤–∏–¥–µ–æ")
            col1, col2 = st.columns([3, 1])
            with col1:
                st.session_state.extraction_step = st.slider(
                    "–®–∞–≥ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ (–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ)",
                    1, 100,
                    key='step_slider',
                    on_change=sync_slider
                )
            with col2:
                st.session_state.extraction_step = st.number_input(
                    " ",
                    min_value=1,
                    max_value=100,
                    key='step_input',
                    on_change=sync_input
                )

            col3, col4 = st.columns([3, 1])
            with col3:
                st.session_state.extraction_yolo_conf = st.slider(
                    "YOLO –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –õ—é–¥–µ–π)",
                    0.1, 0.9,
                    key='extraction_yolo_conf_slider',
                    step=0.05,
                    on_change=sync_yolo_slider
                )
            with col4:
                st.session_state.extraction_yolo_conf = st.number_input(
                    " ",
                    min_value=0.1,
                    max_value=0.9,
                    step=0.05,
                    format="%.2f",
                    key='extraction_yolo_conf_input',
                    on_change=sync_yolo_input
                )

            col5, col6 = st.columns([3, 1])
            with col5:
                st.session_state.extraction_max_crops = st.slider(
                    "–ú–∞–∫—Å. –∫—Ä–æ–ø–æ–≤ –Ω–∞ ID (–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ)",
                    10, 500,
                    step=10,
                    key='extraction_max_crops_slider',
                    on_change=sync_max_crops_slider
                )
            with col6:
                st.session_state.extraction_max_crops = st.number_input(
                    " ",
                    min_value=10,
                    max_value=500,
                    step=10,
                    key='extraction_max_crops_input',
                    on_change=sync_max_crops_input
                )

            if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ä–∞–∑–º–µ—Ç–∫—É (–¥–ª—è –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ)"):
                st.session_state.extraction_complete = False
                crops_dir = "dataset_raw"
                meta_csv = 'crops_meta.csv'

                if os.path.exists(crops_dir):
                    shutil.rmtree(crops_dir)
                if os.path.exists(meta_csv):
                    os.remove(meta_csv)
                
                Path(crops_dir).mkdir(exist_ok=True, parents=True)

                all_id2cnt = {}
                all_meta = []
                
                with st.spinner("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –≤–∏–¥–µ–æ... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è."):
                    for i, video_path in enumerate(video_paths_to_process):
                        st.info(f"–ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ {i+1}/{len(video_paths_to_process)}: {os.path.basename(video_path)}")
                        tid_prefix = f"v{i+1}"
                        id2cnt, meta = extract_people_from_video(
                            video_path,
                            crops_dir,
                            tid_prefix=tid_prefix,
                            step=int(st.session_state.extraction_step),
                            yolo_conf=float(st.session_state.extraction_yolo_conf),
                            max_crops=int(st.session_state.extraction_max_crops)
                        )
                        all_id2cnt.update(id2cnt)
                        all_meta.extend(meta)
                
                if all_meta:
                    pd.DataFrame(all_meta).to_csv(meta_csv, index=False)
                    st.success(
                        f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {sum(all_id2cnt.values())} –∫—Ä–æ–ø–æ–≤ –¥–ª—è {len(all_id2cnt)} —á–µ–ª–æ–≤–µ–∫ –∏–∑ {len(video_paths_to_process)} –≤–∏–¥–µ–æ.")
                    st.session_state.extraction_complete = True
                    st.session_state.labeling_complete = False
                    st.info("–ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É '–†–∞–∑–º–µ—Ç–∫–∞' –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è.")
                else:
                    st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –∏–∑ –≤–∏–¥–µ–æ.")
    
    with tab2:
        st.header("–†–∞–∑–º–µ—Ç–∫–∞ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π –∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤")
        if not hasattr(st.session_state, 'extraction_complete') or not st.session_state.extraction_complete:
            st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ–∫–∏—Ç–µ –ª—é–¥–µ–π –∏–∑ –≤–∏–¥–µ–æ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ù–∞—Å—Ç—Ä–æ–π–∫–∏'")
        elif os.path.exists("dataset_raw"):
            df_labels_result = label_people("dataset_raw", meta_csv='crops_meta.csv', labels_file='labels.csv')
            
            if df_labels_result is not None and not df_labels_result.empty:
                st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ç–∫–∏")
                st.write(df_labels_result['label'].value_counts())
        else:
            st.info("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è 'dataset_raw' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤.")
    
    with tab3:
        st.header("–î–µ—Ç–µ–∫—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ –û—Ç—á–µ—Ç—ã")
        
        st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –î–µ—Ç–µ–∫—Ü–∏–∏")
        col_det1, col_det2 = st.columns([3, 1])
        with col_det1:
            st.session_state.detection_yolo_conf = st.slider(
                "YOLO –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–î–µ—Ç–µ–∫—Ü–∏—è)",
                0.1, 0.9, st.session_state.detection_yolo_conf, 0.05, key='detection_yolo_conf_slider')
        with col_det2:
            st.session_state.detection_yolo_conf = st.number_input(
                " ",
                min_value=0.1,
                max_value=0.9,
                step=0.05,
                format="%.2f",
                key='detection_yolo_conf_input')

        col_det3, col_det4 = st.columns([3, 1])
        with col_det3:
            st.session_state.person_classification_conf = st.slider(
                "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ß–µ–ª–æ–≤–µ–∫–∞ (–°–ø–µ—Ü. –º–æ–¥–µ–ª—å)",
                0.1, 0.99, st.session_state.person_classification_conf, 0.05, key='person_classification_conf_slider')
        with col_det4:
            st.session_state.person_classification_conf = st.number_input(
                "  ",
                min_value=0.1,
                max_value=0.99,
                step=0.05,
                format="%.2f",
                key='person_classification_conf_input')

        col_det5, col_det6 = st.columns([3, 1])
        with col_det5:
            st.session_state.phone_detection_conf = st.slider(
                "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –î–µ—Ç–µ–∫—Ü–∏–∏ –¢–µ–ª–µ—Ñ–æ–Ω–∞ (–°–ø–µ—Ü. –º–æ–¥–µ–ª—å)",
                0.1, 0.99, st.session_state.phone_detection_conf, 0.05, key='phone_detection_conf_slider')
        with col_det6:
            st.session_state.phone_detection_conf = st.number_input(
                "   ",
                min_value=0.1,
                max_value=0.99,
                step=0.05,
                format="%.2f",
                key='phone_detection_conf_input')

        video_paths_to_detect = []
        if hasattr(st.session_state, 'video_path') and st.session_state.video_path:
            video_paths_to_detect.append(st.session_state.video_path)
        if hasattr(st.session_state, 'video_path_1') and st.session_state.video_path_1:
            video_paths_to_detect.append(st.session_state.video_path_1)

        if not video_paths_to_detect:
            st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ '–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ù–∞—Å—Ç—Ä–æ–π–∫–∏'")
        else:
            default_phone_model_path = "yolo_training/phone_detection5/weights/best.pt"
            default_person_classifier_path = "yolo_cls_training/person_classifier5/weights/best.pt"
        
            phone_model_to_use = getattr(st.session_state, 'phone_model_path', default_phone_model_path)
            person_model_to_use = getattr(st.session_state, 'person_model_path', default_person_classifier_path)

            detector = initialize_detector(
                person_model_to_use,
                phone_model_to_use,
                yolo_conf_det=st.session_state.detection_yolo_conf,
                person_class_conf=st.session_state.person_classification_conf,
                phone_det_conf=st.session_state.phone_detection_conf
            )

            if detector is None:
                st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–µ–∫—Ç–æ—Ä. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –º–æ–¥–µ–ª–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.")
            else:
                col1b, col2b = st.columns(2)
                with col1b:
                    start_button = st.button("‚ñ∂Ô∏è –ù–∞—á–∞—Ç—å –¥–µ—Ç–µ–∫—Ü–∏—é")
                with col2b:
                    stop_button = st.button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å")

                placeholders = []
                if len(video_paths_to_detect) == 1:
                    placeholders.append(st.empty())
                else:
                    cols = st.columns(len(video_paths_to_detect))
                    for col in cols:
                        placeholders.append(col.empty())

                if start_button:
                    st.session_state.processing = True
                    st.session_state.violations_log = []
                
                    detectors = [copy.deepcopy(detector) for _ in video_paths_to_detect]
                    st.session_state.tid_manual_labels_maps = [{} for _ in video_paths_to_detect]

                    caps = [cv2.VideoCapture(os.path.abspath(p)) for p in video_paths_to_detect]
                    outs = []
                    output_paths = []
                    
                    for i, cap in enumerate(caps):
                        fps = cap.get(cv2.CAP_PROP_FPS)
                        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                        output_path = f"annotated_video_{i+1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
                        outs.append(cv2.VideoWriter(output_path, fourcc, fps, (width, height)))
                        output_paths.append(output_path)
                    
                    st.session_state.annotated_video_paths = output_paths

                    if not all(c.isOpened() for c in caps):
                        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏.")
                        st.session_state.processing = False
                    else:
                        total_frames = [int(c.get(cv2.CAP_PROP_FRAME_COUNT)) for c in caps]
                        progress_bars = [st.progress(0) for _ in caps]
                        status_texts = [st.empty() for _ in caps]
                        bbox_labels_by_video_and_frame = load_bbox_labels('labels.csv')
                        frame_indices = [0] * len(caps)

                        while st.session_state.get('processing', False):
                            
                            active_streams = 0
                            for i, cap in enumerate(caps):
                                if not cap.isOpened():
                                    continue

                                ret, frame = cap.read()
                                if not ret:
                                    cap.release()
                                    continue
                                
                                active_streams += 1
                                
                                video_prefix = f"v{i+1}" 
                                
                                frame_with_tracking, _, new_violations = process_video_frame(
                                    frame,
                                    detectors[i], 
                                    frame_idx=frame_indices[i],
                                    bbox_labels_by_video_and_frame=bbox_labels_by_video_and_frame,
                                    video_prefix=video_prefix, 
                                    tid_manual_labels_map=st.session_state.tid_manual_labels_maps[i] 
                                )
                                outs[i].write(frame_with_tracking)
                                st.session_state.violations_log.extend(new_violations)

                                frame_rgb = cv2.cvtColor(frame_with_tracking, cv2.COLOR_BGR2RGB)
                                placeholders[i].image(frame_rgb)
                                
                                frame_indices[i] += 1
                                if total_frames[i] > 0:
                                    progress_bars[i].progress(frame_indices[i] / total_frames[i])
                                    status_texts[i].text(f"–í–∏–¥–µ–æ {i+1}: –∫–∞–¥—Ä {frame_indices[i]}/{total_frames[i]}")
                                else:
                                    status_texts[i].text(f"–í–∏–¥–µ–æ {i+1}: –∫–∞–¥—Ä {frame_indices[i]}")

                            if active_streams == 0:
                                st.session_state.processing = False
                                st.info("–í—Å–µ –≤–∏–¥–µ–æ –ø–æ—Ç–æ–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã.")
                                break

                        for cap in caps:
                            if cap.isOpened(): cap.release()
                        for out in outs: out.release()
                        st.session_state.processing = False
                        for i in range(len(caps)):
                            status_texts[i].text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ {i+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

                if stop_button:
                    st.session_state.processing = False
                    st.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")

                if not st.session_state.get('processing', True):
                    if hasattr(st.session_state, 'violations_log') and st.session_state.violations_log:
                        st.success(f"–î–µ—Ç–µ–∫—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(st.session_state.violations_log)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π.")
                        df_violations = pd.DataFrame(st.session_state.violations_log)
                        txt_data = convert_df_to_html_table(df_violations)
                        st.download_button(
                            label="üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç –æ –Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö (HTML)",
                            data=txt_data,
                            file_name=f"violations_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                            mime="text/html",
                        )
                        st.subheader("–°–ø–∏—Å–æ–∫ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π:")
                        st.markdown(txt_data.decode('utf-8'), unsafe_allow_html=True)
                    else:
                        st.info("–ù–∞—Ä—É—à–µ–Ω–∏–π –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±—ã–ª–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –¥–æ –∏—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è.")

                    if hasattr(st.session_state, 'annotated_video_paths'):
                        st.subheader("–ü—Ä–æ—Å–º–æ—Ç—Ä —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ")
                        
                        paths = st.session_state.annotated_video_paths
                        if paths:
                            cols = st.columns(len(paths))

                            for i, path in enumerate(paths):
                                if os.path.exists(path):
                                    with cols[i]:
                                        st.text(os.path.basename(path))
                                        video_file = open(path, 'rb')
                                        video_bytes = video_file.read()
                                        st.video(video_bytes)
                            
if __name__ == "__main__":
    if 'processing' not in st.session_state:
        st.session_state.processing = False
    if 'extraction_complete' not in st.session_state:
        st.session_state.extraction_complete = False
    if 'labeling_complete' not in st.session_state:
        st.session_state.labeling_complete = False
    if 'tid_manual_labels_maps' not in st.session_state: 
        st.session_state.tid_manual_labels_maps = []
    if 'violations_log' not in st.session_state:
        st.session_state.violations_log = []
    if 'annotated_video_paths' not in st.session_state:
        st.session_state.annotated_video_paths = []
    
    if 'video_path' not in st.session_state:
        st.session_state.video_path = None
        st.session_state.uploaded_video_name_front = None
    if 'video_path_1' not in st.session_state:
        st.session_state.video_path_1 = None
        st.session_state.uploaded_video_name_back = None
    if 'person_model_path' not in st.session_state:
        st.session_state.person_model_path = None
        st.session_state.uploaded_person_model_name = None
    if 'phone_model_path' not in st.session_state: 
        st.session_state.phone_model_path = None
        st.session_state.uploaded_phone_model_name = None

    if 'person_model_path_loaded' not in st.session_state: st.session_state.person_model_path_loaded = None
    if 'phone_model_path_loaded' not in st.session_state: st.session_state.phone_model_path_loaded = None
    if 'yolo_conf_det_loaded' not in st.session_state: st.session_state.yolo_conf_det_loaded = None
    if 'person_class_conf_loaded' not in st.session_state: st.session_state.person_class_conf_loaded = None
    if 'phone_det_conf_loaded' not in st.session_state: st.session_state.phone_det_conf_loaded = None


    main() 